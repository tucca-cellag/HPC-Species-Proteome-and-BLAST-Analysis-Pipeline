# profiles/slurm/config.v8+.yaml
# Configured for exclusive use with snakemake-executor-plugin-slurm

# Profile Settings
# To learn more see:
# https://snakemake.readthedocs.io/en/stable/executing/cli.html
executor: slurm
use-conda: true
conda-cleanup-pkgs: tarballs
verbose: true
show-failed-logs: true
retries: 3
rerun-incomplete: true
jobs: 100 # Slurm jobscript size limit
latency-wait: 120
# NCBI API rate limiting: 10 jobs/second (with API key)
# If you don't have an API key, change to 5/1s to avoid rate limiting
max-jobs-per-timespan: 10/1s
configfile: "config/config.yaml"
default-resources:
  slurm_partition: "batch"
  slurm_account: "default"
  runtime: 1440
  mem_mb: 8000
  cpus_per_task: 4
  # Change the following email address to your own email address if you would
  # like this workflow to notify you via email of started/failed/completed
  # slurm jobs
  slurm_extra: '"--mail-type=ALL --mail-user=firstname.lastname@tufts.edu"'

# Cluster-specific resource settings for BLAST operations
# Following Tufts BLAST documentation recommendations
set-resources:
  makeblastdb:
    slurm_partition: "largemem"
    mem_mb: 64000 # 64GB for large database creation
    runtime: 2880 # 48 hours for large database creation
    cpus_per_task: 1 # makeblastdb is single-threaded
  blastp:
    slurm_partition: "largemem"
    mem_mb: 32000 # 32GB for BLAST searches (increased from 16GB)
    runtime: 1440 # 24 hours for BLAST searches
    cpus_per_task: 16 # More CPUs for parallel BLAST
