import os
import yaml


species_csv = config["species_csv"]
query_fasta = config["query_fasta"]
num_shards = int(config.get("num_shards", 8))
threads_per_blast = int(config.get("threads_per_blast", 8))


rule all:
    input:
        "results/analysis_summary.txt",


rule split_queries:
    input:
        queries=query_fasta,
    output:
        expand(f"resources/query_shards/shard_{{i}}.faa", i=range(num_shards)),
        done=f"resources/query_shards/.split_done",
    log:
        "logs/split_queries.log",
    params:
        num_shards=num_shards,
        results_dir=lambda wc, input, output: os.path.dirname(output.done),
        awk_script=lambda wc, input, output: f'BEGIN{{n=0}} /^>/{{f=sprintf("{os.path.dirname(output.done)}/shard_%d.faa",n++%{num_shards});}} {{print > f}}',
    conda:
        "envs/entrez-blast.yaml"
    shell:
        """
        (echo "Starting query splitting..." && \
        echo "Splitting {input.queries} into {params.num_shards} shards" && \
        awk '{params.awk_script}' {input.queries} && \
        echo "Creating completion flag..." && \
        touch {output.done} && \
        echo "Query splitting completed") 2> {log}
        """


checkpoint resolve_accessions:
    input:
        csv=species_csv,
    output:
        status="resources/species_status.csv",
        accessions="resources/accessions.txt",
        download_info="resources/download_info.csv",
    log:
        "logs/resolve_accessions.log",
    threads: lambda wc, input: int(config.get("resolve_accessions_threads", 3))
    conda:
        "envs/ncbi-datasets.yaml"
    shell:
        """
        (set -Eeuo pipefail; \
        echo "[$(date -Is)] Starting resolve_accessions"; \
        echo "Host: $(hostname)"; \
        echo "Species CSV: {input.csv}"; \
        echo "Status out: {output.status}"; \
        echo "Accessions out: {output.accessions}"; \
        echo "Download info out: {output.download_info}"; \
        echo "Threads: {threads}"; \
        PYTHONUNBUFFERED=1 python -u workflow/scripts/resolve_accessions.py \
        --species {input.csv} \
        --status {output.status} \
        --accessions {output.accessions} \
        --download-info {output.download_info} \
        --max-workers {threads}; \
        echo "[$(date -Is)] Finished resolve_accessions") &> {log}
        """


rule datasets_download_proteome:
    input:
        accs="resources/accessions.txt",
        download_info="resources/download_info.csv",
    output:
        dataset_zip="resources/proteome_zips/{acc}.zip",
    wildcard_constraints:
        acc=r"(GCF|GCA)_\d+\.\d+",
    log:
        "logs/download_proteome/{acc}.log",
    conda:
        "envs/ncbi-datasets.yaml"
    message:
        "Downloading proteome for NCBI genome accession: {wildcards.acc}"
    shell:
        """
        (echo "Starting download for {wildcards.acc}" && \
        mkdir -pv ./resources/proteome_zips/ && \
        echo "Downloading proteome data..." && \
        datasets download genome accession {wildcards.acc} \
            --include protein \
            --filename {output.dataset_zip} && \
        echo "Download completed for {wildcards.acc}") &> {log}
        """


rule extract_proteome:
    input:
        dataset_zip="resources/proteome_zips/{acc}.zip",
    output:
        proteome="resources/proteomes/{acc}.faa.gz",
    wildcard_constraints:
        acc=r"(GCF|GCA)_\d+\.\d+",
    log:
        "logs/extract_proteome/{acc}.log",
    conda:
        "envs/ncbi-datasets.yaml"
    message:
        "Extracting proteome for NCBI genome accession: {wildcards.acc}"
    shell:
        """
        (echo "Starting extraction for {wildcards.acc}" && \
        mkdir -pv ./resources/proteomes/ && \
        echo "Extracting protein sequences..." && \
        unzip -p {input.dataset_zip} ncbi_dataset/data/{wildcards.acc}/protein.faa | \
        gzip > {output.proteome} && \
        echo "Extraction completed for {wildcards.acc}") &> {log}
        """


rule concat_proteomes:
    input:
        proteomes=lambda wildcards: expand(
            "resources/proteomes/{acc}.faa.gz",
            acc=[
                line.strip()
                for line in open(
                    checkpoints.resolve_accessions.get().output.accessions
                )
                if line.strip()
            ],
        ),
    output:
        fasta="resources/blast_db/all_species_proteomes.faa",
        done="resources/blast_db/.concat_done",
    log:
        "logs/concat_proteomes.log",
    params:
        length=lambda wildcards, input: len(input.proteomes),
    conda:
        "envs/entrez-blast.yaml"
    shell:
        """
        (echo "Starting proteome concatenation..." && \
        mkdir -p "resources/blast_db" && \
        echo "Concatenating {params.length} proteome files..." && \
        for proteome in {input.proteomes}; do \
            echo "Processing: $proteome" && \
            gunzip -c "$proteome" >> {output.fasta}; \
        done && \
        echo "Creating completion flag..." && \
        touch {output.done} && \
        echo "Proteome concatenation completed") 2> {log}
        """


rule makeblastdb:
    input:
        fasta="resources/blast_db/all_species_proteomes.faa",
    output:
        multiext(
            "resources/blast_db/species_db",
            ".pdb",
            ".phr",
            ".pin",
            ".pot",
            ".psq",
            ".ptf",
            ".pto",
        ),
    log:
        "logs/makeblastdb.log",
    params:
        '-input_type fasta -blastdb_version 5 -title "Custom Species Proteome Database"',
    conda:
        "envs/entrez-blast.yaml"
    wrapper:
        "v7.6.1/bio/blast/makeblastdb"


rule blastp:
    input:
        shard=f"resources/query_shards/shard_{{i}}.faa",
        dbpin=f"resources/blast_db/species_db.pin",
    output:
        tsv=temp(f"resources/blast_results/shard_{{i}}.tsv"),
    log:
        f"logs/blastp/shard_{{i}}.log",
    threads: threads_per_blast
    params:
        blast_db_dir=lambda wc, input, output: os.path.dirname(input.dbpin),
    conda:
        "envs/entrez-blast.yaml"
    shell:
        """
        (echo "Starting BLAST search for shard {wildcards.i}" && \
        echo "Query file: {input.shard}" && \
        echo "Database: {params.blast_db_dir}/species_db" && \
        echo "Output file: {output.tsv}" && \
        blastp -query {input.shard} -db {params.blast_db_dir}/species_db \
            -out {output.tsv} -outfmt '6 qseqid sseqid pident length mismatch \
            gapopen qstart qend sstart send evalue bitscore stitle' \
            -evalue 1e-5 -num_threads {threads} && \
        echo "BLAST search completed for shard {wildcards.i}") 2> {log}
        """


rule aggregate:
    input:
        shards=expand(f"resources/blast_results/shard_{{i}}.tsv", i=range(num_shards)),
        status="resources/species_status.csv",
    output:
        results="results/blast_results.tsv",
        hits="results/species_with_hits.csv",
        summary="results/analysis_summary.txt",
    log:
        "logs/aggregate.log",
    params:
        length=len({input.shards}),
    conda:
        "envs/py.yaml"
    shell:
        """
        (echo "Starting result aggregation..." && \
        echo "Concatenating {params.length} BLAST result files..." && \
        cat {input.shards} > {output.results} && \
        echo "Running analysis script..." && \
        python workflow/scripts/analyze.py \
            --status {input.status} --blast {output.results} \
            --summary {output.summary} --hits {output.hits} && \
        echo "Analysis completed successfully") 2> {log}
        """
